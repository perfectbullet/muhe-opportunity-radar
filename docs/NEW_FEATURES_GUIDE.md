# ğŸ“¦ æ–°åŠŸèƒ½ä½¿ç”¨æŒ‡å—

## ğŸ‰ å·²å®ç°çš„ä¸‰å¤§åŠŸèƒ½

### 1ï¸âƒ£ æ–‡æ¡£è§£æå™¨æ¨¡å—
**ä½ç½®**: `analysis/document_parser.py`

**æ”¯æŒæ ¼å¼**:
- PDFï¼ˆ`.pdf`ï¼‰- ä½¿ç”¨ pdfplumber æˆ– PyPDF2
- Wordï¼ˆ`.doc`, `.docx`ï¼‰- ä½¿ç”¨ python-docx
- Markdownï¼ˆ`.md`, `.markdown`ï¼‰- åŸç”Ÿæ”¯æŒ

**åŸºæœ¬ç”¨æ³•**:
```python
from analysis.document_parser import parse_document

# è§£ææ–‡æ¡£
result = parse_document("è´¢æŠ¥.pdf")

if result["success"]:
    print(f"å†…å®¹: {result['content']}")
    print(f"é¡µæ•°: {result.get('pages')}")
    print(f"å…ƒæ•°æ®: {result.get('metadata')}")
else:
    print(f"é”™è¯¯: {result['error']}")
```

### 2ï¸âƒ£ LangGraph æ•°æ®åˆ†æå·¥ä½œæµ
**ä½ç½®**: `analysis/graph_workflow.py`

**å·¥ä½œæµç¨‹**:
```
è§£ææ–‡æ¡£ â†’ è®¡ç®—æŒ‡æ ‡ â†’ AIåˆ†æ â†’ ç»“æœæ±‡æ€»
```

**åŠŸèƒ½**:
- ğŸ“Š è‡ªåŠ¨æå–è´¢åŠ¡æŒ‡æ ‡ï¼ˆPEã€PBã€ROEã€PEGã€æ¯›åˆ©ç‡ç­‰ï¼‰
- ğŸ”¢ è®¡ç®—è¡ç”ŸæŒ‡æ ‡ï¼ˆPEG = PE / å¢é•¿ç‡ï¼‰
- ğŸ“ˆ ä¼°å€¼è¯„ä¼°ï¼ˆä½ä¼°/åˆç†/é«˜ä¼°ï¼‰
- â­ ä¼ä¸šè´¨é‡è¯„åˆ†ï¼ˆä¼˜ç§€/è‰¯å¥½/ä¸€èˆ¬/è¾ƒå·®ï¼‰
- ğŸ¤– ç»“åˆæŠ•èµ„è€…ç”»åƒè¿›è¡Œ AI æ·±åº¦åˆ†æ

**åŸºæœ¬ç”¨æ³•**:
```python
from analysis.graph_workflow import DataAnalysisWorkflow

# åˆ›å»ºå·¥ä½œæµ
workflow = DataAnalysisWorkflow(llm_provider="siliconflow")

# æ‰§è¡Œåˆ†æ
result = workflow.run(
    material="""
    å…¬å¸ï¼šè´µå·èŒ…å°
    PEï¼š35å€
    ROEï¼š30%
    è¥æ”¶å¢é•¿ï¼š15%
    """,
    investor_id="buffett"
)

# æŸ¥çœ‹ç»“æœ
if result["final_report"]:
    print(result["final_report"]["markdown"])
```

**å¼‚æ­¥ç”¨æ³•**:
```python
# åœ¨ FastAPI ä¸­ä½¿ç”¨
result = await workflow.run_async(
    material=material,
    investor_id="buffett"
)
```

### 3ï¸âƒ£ æ–‡ä»¶ä¸Šä¼  API æ¥å£
**ä½ç½®**: `api/routers/documents.py`

**æ ¸å¿ƒæ¥å£**:

#### ğŸ“¤ ä¸Šä¼ æ–‡æ¡£
```bash
POST /api/v1/documents/upload
Content-Type: multipart/form-data

å‚æ•°:
- file: æ–‡æ¡£æ–‡ä»¶ï¼ˆå¿…éœ€ï¼‰
- investor_id: æŠ•èµ„è€…IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ buffettï¼‰
- auto_analyze: æ˜¯å¦è‡ªåŠ¨åˆ†æï¼ˆå¯é€‰ï¼Œé»˜è®¤ falseï¼‰

å“åº”:
{
  "success": true,
  "document_id": "uuid-xxx",
  "filename": "è´¢æŠ¥.pdf",
  "format": "pdf",
  "size": 1024000,
  "content_preview": "...",
  "metadata": {...}
}
```

#### ğŸ”„ å·¥ä½œæµåˆ†æ
```bash
POST /api/v1/documents/analyze-workflow
Content-Type: application/json

{
  "material": "å…¬å¸ï¼šèŒ…å°\nPEï¼š35\nROEï¼š30%",
  "investor_id": "buffett",
  "use_workflow": true
}

å“åº”:
{
  "success": true,
  "final_report": {
    "markdown": "# æŠ•èµ„åˆ†ææŠ¥å‘Š\n...",
    "structured_data": {...}
  },
  "workflow_result": {...}
}
```

#### ğŸ“„ åˆ†æå·²ä¸Šä¼ æ–‡æ¡£
```bash
POST /api/v1/documents/analyze-document
Content-Type: application/json

{
  "document_id": "uuid-xxx",
  "investor_id": "buffett"
}
```

#### ğŸ“‹ åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£
```bash
GET /api/v1/documents/documents
```

#### ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£
```bash
DELETE /api/v1/documents/documents/{document_id}
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
```bash
pip install langgraph pdfplumber python-docx python-multipart
```

### 2. å¯åŠ¨ API æœåŠ¡
```bash
python -m uvicorn api.main:app --reload --port 8000
```

### 3. è®¿é—® API æ–‡æ¡£
æ‰“å¼€æµè§ˆå™¨: http://localhost:8000/api/docs

### 4. æµ‹è¯•å·¥ä½œæµ
```bash
python scripts/test_workflow.py
```

---

## ğŸ“ æ–°å¢æ–‡ä»¶ç»“æ„

```
analysis/
â”œâ”€â”€ document_parser.py        # æ–‡æ¡£è§£æå™¨
â”œâ”€â”€ graph_workflow.py          # LangGraph å·¥ä½œæµ
â””â”€â”€ nodes/                     # å·¥ä½œæµèŠ‚ç‚¹
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ parse_node.py         # æ–‡æ¡£è§£æèŠ‚ç‚¹
    â”œâ”€â”€ calculate_node.py     # æŒ‡æ ‡è®¡ç®—èŠ‚ç‚¹
    â”œâ”€â”€ analyze_node.py       # AI åˆ†æèŠ‚ç‚¹
    â””â”€â”€ summarize_node.py     # ç»“æœæ±‡æ€»èŠ‚ç‚¹

api/
â”œâ”€â”€ routers/
â”‚   â””â”€â”€ documents.py           # æ–‡æ¡£ç®¡ç† API
â”œâ”€â”€ services/
â”‚   â””â”€â”€ workflow_service.py    # å·¥ä½œæµæœåŠ¡å±‚
â””â”€â”€ models/
    â”œâ”€â”€ requests.py            # æ–°å¢è¯·æ±‚æ¨¡å‹
    â””â”€â”€ responses.py           # æ–°å¢å“åº”æ¨¡å‹

data/
â””â”€â”€ uploads/                   # ä¸Šä¼ æ–‡ä»¶å­˜å‚¨ç›®å½•

scripts/
â””â”€â”€ test_workflow.py           # æµ‹è¯•è„šæœ¬
```

---

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: ä¸Šä¼ å¹¶åˆ†æè´¢æŠ¥ PDF

**ä½¿ç”¨ curl**:
```bash
curl -X POST "http://localhost:8000/api/v1/documents/upload" \
  -F "file=@è´¢æŠ¥.pdf" \
  -F "investor_id=buffett" \
  -F "auto_analyze=true"
```

**ä½¿ç”¨ Python**:
```python
import requests

files = {'file': open('è´¢æŠ¥.pdf', 'rb')}
data = {'investor_id': 'buffett', 'auto_analyze': 'true'}

response = requests.post(
    'http://localhost:8000/api/v1/documents/upload',
    files=files,
    data=data
)

print(response.json())
```

### ç¤ºä¾‹ 2: ä½¿ç”¨å·¥ä½œæµåˆ†ææ–‡æœ¬ææ–™

```python
import requests

payload = {
    "material": """
    å…¬å¸ï¼šå®å¾·æ—¶ä»£
    å¸‚ç›ˆç‡ï¼š45å€
    å¸‚å‡€ç‡ï¼š8å€
    ROEï¼š25%
    è¥æ”¶å¢é•¿ï¼š80%
    æ¯›åˆ©ç‡ï¼š28%
    """,
    "investor_id": "lynch",  # ä½¿ç”¨å½¼å¾—Â·æ—å¥‡çš„è§†è§’ï¼ˆæˆé•¿è‚¡ä¸“å®¶ï¼‰
    "use_workflow": True
}

response = requests.post(
    'http://localhost:8000/api/v1/documents/analyze-workflow',
    json=payload
)

result = response.json()

# æ‰“å°æœ€ç»ˆæŠ¥å‘Š
if result['success']:
    print(result['final_report']['markdown'])
    
    # æŸ¥çœ‹æå–çš„æŒ‡æ ‡
    metrics = result['workflow_result']['calculated_metrics']['metrics']
    print(f"\nPEG æ¯”ç‡: {metrics.get('peg_ratio')}")
```

### ç¤ºä¾‹ 3: åœ¨ Gradio ä¸­é›†æˆå·¥ä½œæµ

```python
import gradio as gr
from analysis.graph_workflow import DataAnalysisWorkflow

workflow = DataAnalysisWorkflow()

def analyze_with_workflow(material, investor_id):
    result = workflow.run(material=material, investor_id=investor_id)
    
    if result.get("final_report"):
        return result["final_report"]["markdown"]
    else:
        return f"åˆ†æå¤±è´¥: {result.get('error')}"

# Gradio ç•Œé¢
demo = gr.Interface(
    fn=analyze_with_workflow,
    inputs=[
        gr.Textbox(label="åˆ†æææ–™", lines=10),
        gr.Dropdown(choices=["buffett", "graham", "lynch"], label="æŠ•èµ„è€…")
    ],
    outputs=gr.Markdown(label="åˆ†ææŠ¥å‘Š")
)

demo.launch()
```

---

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡
ç¡®ä¿ `.env` æ–‡ä»¶åŒ…å«ä»¥ä¸‹é…ç½®ï¼š

```bash
# LLM Providerï¼ˆé»˜è®¤ siliconflowï¼‰
SILICONFLOW_API_KEY=sk-xxx
DEEPSEEK_API_KEY=sk-xxx  # å¯é€‰
QWEN_API_KEY=sk-xxx       # å¯é€‰

# MongoDBï¼ˆå¯é€‰ï¼‰
MONGODB_URI=mongodb://localhost:27017/
MONGODB_DB_NAME=muhe_opportunity_radar
```

### è‡ªå®šä¹‰å·¥ä½œæµèŠ‚ç‚¹

å¦‚æœéœ€è¦æ·»åŠ æ–°çš„åˆ†æèŠ‚ç‚¹ï¼š

1. åœ¨ `analysis/nodes/` åˆ›å»ºæ–°èŠ‚ç‚¹æ–‡ä»¶
2. å®ç°èŠ‚ç‚¹å‡½æ•°ï¼ˆæ¥æ”¶ `state` å­—å…¸ï¼Œè¿”å›æ›´æ–°åçš„ `state`ï¼‰
3. åœ¨ `graph_workflow.py` ä¸­æ·»åŠ èŠ‚ç‚¹å¹¶è¿æ¥

```python
# ç¤ºä¾‹ï¼šæ·»åŠ é£é™©è¯„ä¼°èŠ‚ç‚¹
def risk_assessment_node(state):
    # å®ç°é£é™©è¯„ä¼°é€»è¾‘
    return {
        **state,
        "risk_score": calculate_risk(state)
    }

# åœ¨å·¥ä½œæµä¸­æ·»åŠ 
workflow.add_node("risk_assessment", risk_assessment_node)
workflow.add_edge("analyze", "risk_assessment")
workflow.add_edge("risk_assessment", "summarize")
```

---

## ğŸ“Š å·¥ä½œæµå¯è§†åŒ–

LangGraph å·¥ä½œæµçš„æ‰§è¡Œæµç¨‹ï¼š

```mermaid
graph LR
    A[å¼€å§‹] --> B[è§£ææ–‡æ¡£]
    B --> C[è®¡ç®—æŒ‡æ ‡]
    C --> D[AI åˆ†æ]
    D --> E[ç»“æœæ±‡æ€»]
    E --> F[ç»“æŸ]
    
    B -.æå–æ–‡æœ¬.-> C
    C -.è®¡ç®—PE/ROE/PEG.-> D
    D -.æŠ•èµ„è€…è§†è§’åˆ†æ.-> E
    E -.ç”ŸæˆMarkdownæŠ¥å‘Š.-> F
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **PDF è§£æ**: éœ€è¦å®‰è£… `pdfplumber`ï¼ˆæ¨èï¼‰æˆ– `PyPDF2`
2. **LangGraph**: å¿…é¡»å®‰è£… `pip install langgraph`
3. **æ–‡ä»¶ä¸Šä¼ **: ä¸Šä¼ çš„æ–‡ä»¶ä¿å­˜åœ¨ `data/uploads/` ç›®å½•
4. **LLM è°ƒç”¨**: ç¡®ä¿é…ç½®äº†æœ‰æ•ˆçš„ API Key
5. **å¼‚æ­¥å¤„ç†**: å·¥ä½œæµåœ¨åç«¯ä½¿ç”¨ `asyncio.to_thread` é¿å…é˜»å¡

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: LangGraph æœªå®‰è£…
```bash
pip install langgraph
```

### Q2: PDF è§£æå¤±è´¥
```bash
pip install pdfplumber
# æˆ–
pip install PyPDF2
```

### Q3: Word æ–‡æ¡£è§£æå¤±è´¥
```bash
pip install python-docx
```

### Q4: æ–‡ä»¶ä¸Šä¼  413 é”™è¯¯ï¼ˆæ–‡ä»¶è¿‡å¤§ï¼‰
åœ¨ `uvicorn` å¯åŠ¨æ—¶è®¾ç½®ï¼š
```python
uvicorn.run(app, limit_max_size=100*1024*1024)  # 100MB
```

---

## ğŸ“ è¿›é˜¶ç”¨æ³•

### æ·»åŠ è‡ªå®šä¹‰è´¢åŠ¡æŒ‡æ ‡

ä¿®æ”¹ `analysis/nodes/calculate_node.py`ï¼š

```python
def calculate_metrics_node(state):
    # æ·»åŠ æ–°æŒ‡æ ‡æå–
    metrics = {
        "pe_ratio": _extract_metric(text, r"PE[ï¼š:=\s]*(\d+\.?\d*)"),
        # æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡
        "debt_ratio": _extract_metric(text, r"è´Ÿå€ºç‡[ï¼š:=\s]*(\d+\.?\d*)%?"),
        "quick_ratio": _extract_metric(text, r"é€ŸåŠ¨æ¯”ç‡[ï¼š:=\s]*(\d+\.?\d*)"),
    }
    
    # è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘
    if metrics["debt_ratio"] and metrics["debt_ratio"] < 30:
        # ä½è´Ÿå€ºç‡åŠ åˆ†
        pass
    
    return {**state, "calculated_metrics": metrics}
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [FastAPI æ–‡ä»¶ä¸Šä¼ ](https://fastapi.tiangolo.com/tutorial/request-files/)
- [pdfplumber æ–‡æ¡£](https://github.com/jsvine/pdfplumber)

---

âœ… **æ‰€æœ‰åŠŸèƒ½å·²å®Œæ•´å®ç°ï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨ï¼**
